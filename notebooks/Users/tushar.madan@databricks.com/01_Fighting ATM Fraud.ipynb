{"cells":[{"cell_type":"markdown","source":["*** According to the Secret Service, the crime is responsible for about $350,000 of monetary losses each day in the United States and is considered to be the number one ATM-related crime. Trade group Global ATM Security Alliance estimates that skimming costs the U.S.-banking industry about $60 million a year.***\n\nThe easiest way that companies identify atm fraud is by recognizing a break in spending patterns.  For example, if you live in Wichita, KS and suddenly your card is used to buy something in Bend, OR â€“ that may tip the scales in favor of possible fraud, and your credit card company might decline the charges and ask you to verify them.\n\n![Databricks for Credit Card Fraud](https://thephp.cc/images/news/credit_card_with_padlock.jpg \"Databricks for Credit Card Fraud\")  \n* [**ATM Fraud Analytics**](https://www.csoonline.com/article/2124891/fraud-prevention/atm-skimming--how-to-recognize-card-fraud.html) is the use of data analytics and machine learning to detect ATM fraud and is...  \n  * Built on top of Databricks Platform\n  * Uses a machine learning implementation to detect ATM fraud   \n* This demo...  \n  * demonstrates a ATM fraud detection workflow.  The dataset we use is internally mocked up data."],"metadata":{}},{"cell_type":"markdown","source":["# Pipeline from Event Hubs / Kafka / Azure Storage \n\nApache Spark 2.0 adds the first version of a new higher-level stream processing API, Structured Streaming. In this notebook we are going to take a quick look at how to use DataFrame API to build Structured Streaming applications. At large organizations it is typical for different teams to read and write streaming data. In this example, we are going to load JSON data that has been partitioned by timestamp into a DataFrame stream. Next, we want to compute real-time metrics like running counts and windowed counts on a stream of timestamped actions.\n\n![stream](https://s3-us-west-2.amazonaws.com/db-jodwyer/images/atmFraudAws.png)"],"metadata":{}},{"cell_type":"markdown","source":["###Step 1: Explore Data"],"metadata":{}},{"cell_type":"code","source":["%sql use fraud_demo;\ncache table atm_visits;"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%sql select * from atm_visits limit 100"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%sql select count(*) from atm_visits "],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%sql select sum(amount), month, fraud_report from atm_visits where year = $year group by month, fraud_report order by month, fraud_report"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sql select sum(amount), month, fraud_report from atm_visits where year = $year group by month, fraud_report order by month, fraud_report"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["###Step 2 and 3: Enrich Data, Mask Card Numbers and Visualize"],"metadata":{}},{"cell_type":"code","source":["%sql create or replace view atm_dataset as \n  select concat(substr(card_number,0,4), '********',substr(card_number,-4)) masked_card_number, c.checking_savings, c.first_name, c.last_name, c.customer_since_date, c.customer_id, v.*, l.* \n    from atm_customers c \n      inner join  \n        atm_visits v using (customer_id) \n      inner join \n        atm_locations l using (atm_id)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%sql select * from atm_dataset"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sql select count(1), city_state_zip.state state from atm_dataset where year = 2016 and fraud_report = 'Y' group by city_state_zip.state"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["###Step 4 and 5: Create & Use Spark ML Model"],"metadata":{}},{"cell_type":"markdown","source":["### Workflows with Pyspark.ML Pipeline\n<img src=\"https://s3-us-west-2.amazonaws.com/pub-tc/ML-workflow.png\" width=\"800\">"],"metadata":{}},{"cell_type":"markdown","source":["##Model Export / Import\n\nDatabricks ML Model Export allows you to export models and full ML pipelines from Apache Spark. These exported models and pipelines can be imported into other (Spark and non-Spark) platforms to do scoring and make predictions. Model Export is targeted at low-latency, lightweight ML-powered applications\n\nChoose the best Model and Export\n```python\nmodel.bestModel.write().overwrite().save(\"/models/atm_fraud\")```\n\nDocumentation can be found [here](https://docs.azuredatabricks.net/spark/latest/mllib/index.html)"],"metadata":{}},{"cell_type":"code","source":["%scala\n\nimport org.apache.spark.ml.PipelineModel\nimport org.apache.spark.ml.classification.DecisionTreeClassificationModel\nval model = PipelineModel.load(\"/mnt/jodwyer/atm/model/\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%scala\nval dtModel = model.stages.last.asInstanceOf[DecisionTreeClassificationModel]\ndisplay(dtModel)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["### Step 6: Stream incoming data and score fraud in near real-time"],"metadata":{}},{"cell_type":"code","source":["%run ./helpers"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["from pyspark.sql.types  import *\n\njson_schema = StructType([StructField(\"atm_id\",LongType(),True),StructField(\"customer_id\",LongType(),True),StructField(\"visit_id\",LongType(),True),StructField(\"withdrawl_or_deposit\",StringType(),True),StructField(\"amount\",LongType(),True),StructField(\"fraud_report\",StringType(),True),StructField(\"day\",LongType(),True),StructField(\"month\",LongType(),True),StructField(\"year\",LongType(),True),StructField(\"hour\",LongType(),True),StructField(\"min\",LongType(),True),StructField(\"sec\",LongType(),True),StructField(\"card_number\",LongType(),True),StructField(\"checking_savings\",StringType(),True),StructField(\"first_name\",StringType(),True),StructField(\"last_name\",StringType(),True),StructField(\"customer_since_date\",DateType(),True),StructField(\"city_state_zip\",StructType([StructField(\"city\",StringType(),True),StructField(\"state\",StringType(),True),StructField(\"zip\",StringType(),True)]),True),StructField(\"pos_capability\",StringType(),True),StructField(\"offsite_or_onsite\",StringType(),True),StructField(\"bank\",StringType(),True)])"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["%fs ls /mnt/jodwyer/atm/json"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["from pyspark.ml import *\n\n\nstreamingInputDF = spark.readStream.schema(json_schema).option(\"maxFilesPerTrigger\", 1).json(\"/mnt/jodwyer/atm/json\")\n\nstreamingFraudDataset = streamingInputDF.selectExpr(\"to_sql_timestamp(year, month, day, hour, min, sec) as timestamp\", \"withdrawl_or_deposit\", \"amount\", \"day\", \"month\", \"year\", \"hour\", \"min\", \"checking_savings\", \"pos_capability\", \"offsite_or_onsite\", \"bank\", \"fraud_report\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["display(streamingFraudDataset)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["model = PipelineModel.load(\"/mnt/jodwyer/atm/model/\")"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["model.transform(streamingFraudDataset).createOrReplaceTempView(\"predictions\")"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%sql select timestamp, amount, bank, withdrawl_or_deposit, prediction, getFraudProbability(probability) fraud_probability from predictions "],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%sql select sum(amount), withdrawl_or_deposit, prediction from predictions group by withdrawl_or_deposit, prediction"],"metadata":{},"outputs":[],"execution_count":27}],"metadata":{"name":"01 Fighting ATM Fraud (1)","notebookId":3121245},"nbformat":4,"nbformat_minor":0}